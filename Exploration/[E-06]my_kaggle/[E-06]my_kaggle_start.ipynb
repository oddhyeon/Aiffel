{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.backend' has no attribute 'set_session'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ce6dda48f109>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m                               inter_op_parallelism_threads=1)\n\u001b[1;32m     64\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession_conf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_numeric_for_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_field\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'keras.backend' has no attribute 'set_session'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import time\n",
    "import gc\n",
    "import random as rn\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from datetime import date, datetime, timedelta\n",
    "from functools import wraps\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import ElasticNet, Lasso, Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cat\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Embedding, Reshape, Concatenate, Input, Flatten\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew #for some statistics\n",
    "from scipy.cluster import hierarchy as hc\n",
    "from scipy.special import boxcox1p\n",
    "from keras.models import Sequential\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "#RANDOM_SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "RANDOM_SEED = 42\n",
    "rn.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                              inter_op_parallelism_threads=1)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "def plot_numeric_for_regression(df, field, target_field='price'):\n",
    "    df = df[df[field].notnull()]\n",
    "\n",
    "    fig = plt.figure(figsize = (16, 7))\n",
    "    ax1 = plt.subplot(121)\n",
    "    \n",
    "    sns.distplot(df[df['data'] == 'train'][field], label='Train', hist_kws={'alpha': 0.5}, ax=ax1)\n",
    "    sns.distplot(df[df['data'] == 'test'][field], label='Test', hist_kws={'alpha': 0.5}, ax=ax1)\n",
    "\n",
    "    plt.xlabel(field)\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    \n",
    "    ax2 = plt.subplot(122)\n",
    "    \n",
    "    df_copy = df[df['data'] == 'train'].copy()\n",
    "\n",
    "    sns.scatterplot(x=field, y=target_field, data=df_copy, ax=ax2)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def plot_categorical_for_regression(df, field, target_field='price', show_missing=True, missing_value='NA'):\n",
    "    df_copy = df.copy()\n",
    "    if show_missing: df_copy[field] = df_copy[field].fillna(missing_value)\n",
    "    df_copy = df_copy[df_copy[field].notnull()]\n",
    "\n",
    "    ax1_param = 121\n",
    "    ax2_param = 122\n",
    "    fig_size = (16, 7)\n",
    "    if df_copy[field].nunique() > 30:\n",
    "        ax1_param = 211\n",
    "        ax2_param = 212\n",
    "        fig_size = (16, 10)\n",
    "    \n",
    "    fig = plt.figure(figsize = fig_size)\n",
    "    ax1 = plt.subplot(ax1_param)\n",
    "    \n",
    "    sns.countplot(x=field, hue='data', order=np.sort(df_copy[field].unique()), data=df_copy)\n",
    "    plt.xticks(rotation=90, fontsize=11)\n",
    "    \n",
    "    ax2 = plt.subplot(ax2_param)\n",
    "    \n",
    "    df_copy = df_copy[df_copy['data'] == 'train']\n",
    "\n",
    "    sns.boxplot(x=field, y=target_field, data=df_copy, order=np.sort(df_copy[field].unique()), ax=ax2)\n",
    "    plt.xticks(rotation=90, fontsize=11)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def get_prefix(group_col, target_col, prefix=None):\n",
    "    if isinstance(group_col, list) is True:\n",
    "        g = '_'.join(group_col)\n",
    "    else:\n",
    "        g = group_col\n",
    "    if isinstance(target_col, list) is True:\n",
    "        t = '_'.join(target_col)\n",
    "    else:\n",
    "        t = target_col\n",
    "    if prefix is not None:\n",
    "        return prefix + '_' + g + '_' + t\n",
    "    return g + '_' + t\n",
    "    \n",
    "def groupby_helper(df, group_col, target_col, agg_method, prefix_param=None):\n",
    "    try:\n",
    "        prefix = get_prefix(group_col, target_col, prefix_param)\n",
    "        #print(group_col, target_col, agg_method)\n",
    "        group_df = df.groupby(group_col)[target_col].agg(agg_method)\n",
    "        group_df.columns = ['{}_{}'.format(prefix, m) for m in agg_method]\n",
    "    except BaseException as e:\n",
    "        print(e)\n",
    "    return group_df.reset_index()\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def rmse_exp(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(np.expm1(y_true), np.expm1(y_pred)))\n",
    "\n",
    "def time_decorator(func): \n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print(\"\\nStartTime: \", datetime.now() + timedelta(hours=9))\n",
    "        start_time = time.time()\n",
    "        \n",
    "        df = func(*args, **kwargs)\n",
    "        \n",
    "        print(\"EndTime: \", datetime.now() + timedelta(hours=9))  \n",
    "        print(\"TotalTime: \", time.time() - start_time)\n",
    "        return df\n",
    "        \n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SklearnWrapper(object):\n",
    "    def __init__(self, clf, params=None, **kwargs):\n",
    "        #if isinstance(SVR) is False:\n",
    "        #    params['random_state'] = kwargs.get('seed', 0)\n",
    "        self.clf = clf(**params)\n",
    "        self.is_classification_problem = True\n",
    "        self.use_avg_oof = kwargs.get('use_avg_oof', False)\n",
    "    @time_decorator\n",
    "    def train(self, x_train, y_train, x_cross=None, y_cross=None):\n",
    "        if len(np.unique(y_train)) > 30:\n",
    "            self.is_classification_problem = False\n",
    "            \n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        if self.is_classification_problem is True:\n",
    "            return self.clf.predict_proba(x)[:,1]\n",
    "        else:\n",
    "            return self.clf.predict(x)\n",
    "    \n",
    "class XgbWrapper(object):\n",
    "    def __init__(self, params=None, **kwargs):\n",
    "        self.param = params\n",
    "        self.use_avg_oof = kwargs.get('use_avg_oof', False)\n",
    "        self.num_rounds = kwargs.get('num_rounds', 1000)\n",
    "        self.early_stopping = kwargs.get('ealry_stopping', 100)\n",
    "\n",
    "        self.eval_function = kwargs.get('eval_function', None)\n",
    "        self.verbose_eval = kwargs.get('verbose_eval', 100)\n",
    "        self.best_round = 0\n",
    "    \n",
    "    @time_decorator\n",
    "    def train(self, x_train, y_train, x_cross=None, y_cross=None):\n",
    "        need_cross_validation = True\n",
    "       \n",
    "        if isinstance(y_train, pd.DataFrame) is True:\n",
    "            y_train = y_train[y_train.columns[0]]\n",
    "            if y_cross is not None:\n",
    "                y_cross = y_cross[y_cross.columns[0]]\n",
    "\n",
    "        if x_cross is None:\n",
    "            dtrain = xgb.DMatrix(x_train, label=y_train, silent= True)\n",
    "            train_round = self.best_round\n",
    "            if self.best_round == 0:\n",
    "                train_round = self.num_rounds\n",
    "            \n",
    "            self.clf = xgb.train(self.param, dtrain, train_round)\n",
    "            del dtrain\n",
    "        else:\n",
    "            dtrain = xgb.DMatrix(x_train, label=y_train, silent=True)\n",
    "            dvalid = xgb.DMatrix(x_cross, label=y_cross, silent=True)\n",
    "            watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "\n",
    "            self.clf = xgb.train(self.param, dtrain, self.num_rounds, watchlist, feval=self.eval_function,\n",
    "                                 early_stopping_rounds=self.early_stopping,\n",
    "                                 verbose_eval=self.verbose_eval)\n",
    "            self.best_round = max(self.best_round, self.clf.best_iteration)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(xgb.DMatrix(x), ntree_limit=self.best_round)\n",
    "\n",
    "    def get_params(self):\n",
    "        return self.param    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LgbmWrapper(object):\n",
    "    def __init__(self, params=None, **kwargs):\n",
    "        self.param = params\n",
    "        self.use_avg_oof = kwargs.get('use_avg_oof', False)\n",
    "        self.num_rounds = kwargs.get('num_rounds', 1000)\n",
    "        self.early_stopping = kwargs.get('ealry_stopping', 100)\n",
    "\n",
    "        self.eval_function = kwargs.get('eval_function', None)\n",
    "        self.verbose_eval = kwargs.get('verbose_eval', 100)\n",
    "        self.best_round = 0\n",
    "        \n",
    "    @time_decorator\n",
    "    def train(self, x_train, y_train, x_cross=None, y_cross=None):\n",
    "        \"\"\"\n",
    "        x_cross or y_cross is None\n",
    "        -> model train limted num_rounds\n",
    "        \n",
    "        x_cross and y_cross is Not None\n",
    "        -> model train using validation set\n",
    "        \"\"\"\n",
    "        if isinstance(y_train, pd.DataFrame) is True:\n",
    "            y_train = y_train[y_train.columns[0]]\n",
    "            if y_cross is not None:\n",
    "                y_cross = y_cross[y_cross.columns[0]]\n",
    "\n",
    "        if x_cross is None:\n",
    "            dtrain = lgb.Dataset(x_train, label=y_train, silent= True)\n",
    "            train_round = self.best_round\n",
    "            if self.best_round == 0:\n",
    "                train_round = self.num_rounds\n",
    "                \n",
    "            self.clf = lgb.train(self.param, train_set=dtrain, num_boost_round=train_round)\n",
    "            del dtrain   \n",
    "        else:\n",
    "            dtrain = lgb.Dataset(x_train, label=y_train, silent=True)\n",
    "            dvalid = lgb.Dataset(x_cross, label=y_cross, silent=True)\n",
    "            self.clf = lgb.train(self.param, train_set=dtrain, num_boost_round=self.num_rounds, valid_sets=[dtrain, dvalid],\n",
    "                                  feval=self.eval_function, early_stopping_rounds=self.early_stopping,\n",
    "                                  verbose_eval=self.verbose_eval)\n",
    "            self.best_round = max(self.best_round, self.clf.best_iteration)\n",
    "            del dtrain, dvalid\n",
    "            \n",
    "        gc.collect()\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x, num_iteration=self.clf.best_iteration)\n",
    "    \n",
    "    def plot_importance(self, importance_type='gain', max_num_features=20):\n",
    "        lgb.plot_importance(self.clf, importance_type=importance_type, max_num_features=max_num_features, height=0.7, figsize=(10,30))\n",
    "        plt.show()\n",
    "        \n",
    "    def get_params(self):\n",
    "        return self.param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time_decorator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5f89cef8fbca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mCatWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_avg_oof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'use_avg_oof'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_rounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'num_rounds'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-5f89cef8fbca>\u001b[0m in \u001b[0;36mCatWrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_round\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mtime_decorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cross\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cross\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \"\"\"\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time_decorator' is not defined"
     ]
    }
   ],
   "source": [
    "class CatWrapper(object):\n",
    "    def __init__(self, params=None, **kwargs):\n",
    "        self.param = params\n",
    "        self.use_avg_oof = kwargs.get('use_avg_oof', False)\n",
    "        self.num_rounds = kwargs.get('num_rounds', 1000)\n",
    "        self.param['iterations'] = kwargs.get('num_rounds', 1000)\n",
    "        self.early_stopping = kwargs.get('ealry_stopping', 100)\n",
    "\n",
    "        self.eval_function = kwargs.get('eval_function', None)\n",
    "        self.verbose_eval = kwargs.get('verbose_eval', 100)\n",
    "        self.best_round = 0\n",
    "        \n",
    "    @time_decorator\n",
    "    def train(self, x_train, y_train, x_cross=None, y_cross=None, cat_features=None):\n",
    "        \"\"\"\n",
    "        x_cross or y_cross is None\n",
    "        -> model train limted num_rounds\n",
    "        \n",
    "        x_cross and y_cross is Not None\n",
    "        -> model train using validation set\n",
    "        \"\"\"\n",
    "        if isinstance(y_train, pd.DataFrame) is True:\n",
    "            y_train = y_train[y_train.columns[0]]\n",
    "            if y_cross is not None:\n",
    "                y_cross = y_cross[y_cross.columns[0]]\n",
    "\n",
    "        if x_cross is None:\n",
    "            dtrain = cat.Pool(x_train, y_train, cat_features=cat_features)\n",
    "            train_round = self.best_round\n",
    "            if self.best_round == 0:\n",
    "                train_round = self.num_rounds\n",
    "                \n",
    "            self.clf = cat.CatBoost(params=self.param)\n",
    "            self.clf.fit(dtrain, verbose_eval=self.verbose_eval)\n",
    "            del dtrain   \n",
    "        else:\n",
    "            dtrain = cat.Pool(x_train, y_train, cat_features=cat_features)\n",
    "            dvalid = cat.Pool(x_cross, y_cross, cat_features=cat_features)\n",
    "            \n",
    "            self.clf = cat.CatBoost(params=self.param)\n",
    "            self.clf.fit(dtrain, eval_set=[dvalid], early_stopping_rounds=self.early_stopping, verbose_eval=self.verbose_eval)\n",
    "            self.best_round = max(self.best_round, self.clf.best_iteration_)\n",
    "            del dtrain, dvalid\n",
    "            \n",
    "        gc.collect()\n",
    "    \n",
    "    def predict(self, x):\n",
    "        if self.clf.best_iteration_ is None: return self.clf.predict(x, ntree_end=self.best_round)\n",
    "        else: return self.clf.predict(x, ntree_end=self.clf.best_iteration_)\n",
    "        \n",
    "    def get_params(self):\n",
    "        return self.param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasWrapper(object):\n",
    "    def __init__(self, model_func, params=None, **kwargs):\n",
    "        self.model_func = model_func\n",
    "        self.param = params\n",
    "        self.use_avg_oof = kwargs.get('use_avg_oof', False)\n",
    "        self.epochs = kwargs.get('epochs', 20)\n",
    "        self.batch_size = kwargs.get('batch_size', 16)\n",
    "        self.callbacks = kwargs.get('callbacks', None)\n",
    "        self.shuffle = kwargs.get('shuffle', True)\n",
    "        self.best_epochs = 0\n",
    "    @time_decorator\n",
    "    def train(self, x_train, y_train, x_cross=None, y_cross=None):\n",
    "        self.model = self.model_func(x_train.shape[1])\n",
    "        if x_cross is None:\n",
    "            train_epochs = self.best_epochs\n",
    "            if self.best_epochs == 0:\n",
    "                train_epochs = self.epochs\n",
    "                \n",
    "            self.model.fit(x_train, y_train, epochs=train_epochs, batch_size=self.batch_size,\n",
    "                           shuffle=self.shuffle, callbacks=self.callbacks, verbose=0)\n",
    "        else:\n",
    "            hist = self.model.fit(x_train, y_train, epochs=self.epochs, batch_size=self.batch_size,\n",
    "                                  shuffle=self.shuffle, validation_data=(x_cross, y_cross),\n",
    "                                  callbacks=self.callbacks, verbose=0)\n",
    "            self.best_epochs = max(self.best_epochs, len(hist.history['val_loss']))\n",
    "\n",
    "    def predict(self, x):\n",
    "        if isinstance(x, pd.DataFrame):\n",
    "            return self.model.predict(x.values).ravel()\n",
    "        else:\n",
    "            return self.model.predict(x).ravel()\n",
    "    \n",
    "    def get_params(self):\n",
    "        return self.param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time_decorator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-441334e274ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mKerasEmbeddingWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_avg_oof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'use_avg_oof'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-441334e274ff>\u001b[0m in \u001b[0;36mKerasEmbeddingWrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shuffle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mtime_decorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cross\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cross\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mnon_embedding_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time_decorator' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "class KerasEmbeddingWrapper(object):\n",
    "    def __init__(self, model_func, params=None, **kwargs):\n",
    "        self.model_func = model_func\n",
    "        self.param = params\n",
    "        self.use_avg_oof = kwargs.get('use_avg_oof', False)\n",
    "        self.epochs = kwargs.get('epochs', 20)\n",
    "        self.batch_size = kwargs.get('batch_size', 16)\n",
    "        self.callbacks = kwargs.get('callbacks', None)\n",
    "        self.embedding_cols = kwargs.get('embedding_cols', None)\n",
    "        self.shuffle = kwargs.get('shuffle', True)\n",
    "        self.best_epochs = 0\n",
    "    @time_decorator\n",
    "    def train(self, x_train, y_train, x_cross=None, y_cross=None):\n",
    "        non_embedding_cols = [col for col in x_train.columns if col not in self.embedding_cols]\n",
    "        self.model = self.model_func(x_train, self.embedding_cols)\n",
    "        if x_cross is None:\n",
    "            train_epochs = self.best_epochs\n",
    "            if self.best_epochs == 0:\n",
    "                train_epochs = self.epochs\n",
    "                \n",
    "            x_tr_list = []\n",
    "            x_tr_list.append(x_train[non_embedding_cols])\n",
    "            for col in self.embedding_cols:\n",
    "                x_tr_list.append(x_train[col])\n",
    "            self.model.fit(x_tr_list, y_train, epochs=train_epochs, batch_size=self.batch_size,\n",
    "                           shuffle=self.shuffle, callbacks=self.callbacks, verbose=0)\n",
    "        else:\n",
    "            x_tr_list = []\n",
    "            x_tr_list.append(x_train[non_embedding_cols])\n",
    "            for col in self.embedding_cols:\n",
    "                x_tr_list.append(x_train[col])\n",
    "\n",
    "            x_cr_list = []\n",
    "            x_cr_list.append(x_cross[non_embedding_cols])\n",
    "            for col in self.embedding_cols:\n",
    "                x_cr_list.append(x_cross[col])\n",
    "            hist = self.model.fit(x_tr_list, y_train, epochs=self.epochs, batch_size=self.batch_size, shuffle=self.shuffle,\n",
    "                                  validation_data=(x_cr_list, y_cross), callbacks=self.callbacks, verbose=0)\n",
    "            self.best_epochs = max(self.best_epochs, len(hist.history['val_loss']))\n",
    "\n",
    "    def predict(self, x):\n",
    "        non_embedding_cols = [col for col in x.columns if col not in self.embedding_cols]\n",
    "        x_list = []\n",
    "        x_list.append(x[non_embedding_cols])\n",
    "        for col in self.embedding_cols:\n",
    "            x_list.append(x[col])\n",
    "        return self.model.predict(x_list).ravel()\n",
    "    \n",
    "    def get_params(self):\n",
    "        return self.param\n",
    "\n",
    "\n",
    "@time_decorator\n",
    "def get_oof(clf, x_train, y_train, x_test, eval_func, **kwargs):\n",
    "    nfolds = kwargs.get('NFOLDS', 5)\n",
    "    kfold_shuffle = kwargs.get('kfold_shuffle', True)\n",
    "    kfold_random_state = kwargs.get('kfold_random_state', 0)\n",
    "    stratified_kfold_ytrain = kwargs.get('stratifed_kfold_y_value', None)\n",
    "    ntrain = x_train.shape[0]\n",
    "    ntest = x_test.shape[0]\n",
    "    \n",
    "    kf_split = None\n",
    "    if stratified_kfold_ytrain is None:\n",
    "        kf = KFold(n_splits=nfolds, shuffle=kfold_shuffle, random_state=kfold_random_state)\n",
    "        kf_split = kf.split(x_train)\n",
    "    else:\n",
    "        kf = StratifiedKFold(n_splits=nfolds, shuffle=kfold_shuffle, random_state=kfold_random_state)\n",
    "        kf_split = kf.split(x_train, stratified_kfold_ytrain)\n",
    "        \n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "\n",
    "    cv_sum = 0\n",
    "    \n",
    "    # before running model, print model param\n",
    "    # lightgbm model and xgboost model use get_params()\n",
    "    try:\n",
    "        if clf.clf is not None:\n",
    "            print(clf.clf)\n",
    "    except:\n",
    "        print(clf)\n",
    "        print(clf.get_params())\n",
    "\n",
    "    for i, (train_index, cross_index) in enumerate(kf_split):\n",
    "        x_tr, x_cr = None, None\n",
    "        y_tr, y_cr = None, None\n",
    "        if isinstance(x_train, pd.DataFrame):\n",
    "            x_tr, x_cr = x_train.iloc[train_index], x_train.iloc[cross_index]\n",
    "            y_tr, y_cr = y_train.iloc[train_index], y_train.iloc[cross_index]\n",
    "        else:\n",
    "            x_tr, x_cr = x_train[train_index], x_train[cross_index]\n",
    "            y_tr, y_cr = y_train[train_index], y_train[cross_index]\n",
    "\n",
    "        clf.train(x_tr, y_tr, x_cr, y_cr)\n",
    "        \n",
    "        oof_train[cross_index] = clf.predict(x_cr)\n",
    "        if hasattr(clf, 'use_avg_oof') and clf.use_avg_oof:\n",
    "            oof_test += clf.predict(x_test)/nfolds\n",
    "\n",
    "        cv_score = eval_func(y_cr, oof_train[cross_index])\n",
    "        \n",
    "        print('Fold %d / ' % (i+1), 'CV-Score: %.6f' % cv_score)\n",
    "        cv_sum = cv_sum + cv_score\n",
    "        \n",
    "        del x_tr, x_cr, y_tr, y_cr\n",
    "        \n",
    "    gc.collect()\n",
    "    \n",
    "    score = cv_sum / nfolds\n",
    "    print(\"Average CV-Score: \", score)\n",
    "\n",
    "    # Using All Dataset, retrain\n",
    "    if not hasattr(clf, 'use_avg_oof') or clf.use_avg_oof is False:\n",
    "        clf.train(x_train, y_train)\n",
    "        oof_test = clf.predict(x_test)\n",
    "\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1), score\n",
    "\n",
    "@time_decorator\n",
    "def stacking(data_list, y_train, model_list, eval_func=None, nfolds=5, kfold_random_state=RANDOM_SEED):\n",
    "    \n",
    "    oof_train_list = []\n",
    "    oof_test_list = []\n",
    "    oof_cv_score_list = []\n",
    "    \n",
    "    for X_train, X_test in data_list:\n",
    "        print(X_train.shape, X_test.shape, y_train.shape)\n",
    "        for model in model_list:\n",
    "            oof_train, oof_test, oof_cv_score = get_oof(model, X_train, y_train, X_test, eval_func,\n",
    "                                                        NFOLDS=nfolds, kfold_random_state=kfold_random_state)\n",
    "            oof_train_list.append(oof_train)\n",
    "            oof_test_list.append(oof_test)\n",
    "            oof_cv_score_list.append(oof_cv_score)\n",
    "        \n",
    "    X_train_next = pd.DataFrame(np.concatenate(oof_train_list, axis=1))\n",
    "    X_test_next = pd.DataFrame(np.concatenate(oof_test_list, axis=1))\n",
    "    \n",
    "    print(X_train_next.shape, X_test_next.shape)\n",
    "    \n",
    "    return X_train_next, X_test_next, oof_cv_score_list\n",
    "\n",
    "\n",
    "def load_data(nb_1km=True, nb_3km=True, nb_5km=True,\n",
    "              n_5_nb=True, n_10_nb=True, n_20_nb=True,\n",
    "              original=True, do_scale=False, fix_skew=False, do_ohe=True):\n",
    "    train = pd.read_csv('aiffel/kaggle_kakr_housing/data/train.csv')\n",
    "    test = pd.read_csv('aiffel/kaggle_kakr_housing/data/test.csv')\n",
    "\n",
    "    train_copy = train.copy()\n",
    "    train_copy['data'] = 'train'\n",
    "    test_copy = test.copy()\n",
    "    test_copy['data'] = 'test'\n",
    "    test_copy['price'] = np.nan\n",
    "    \n",
    "    # remove outlier\n",
    "    train_copy = train_copy[~((train_copy['sqft_living'] > 12000) & (train_copy['price'] < 3000000))].reset_index(drop=True)\n",
    "\n",
    "    # concat train, test data to preprocess\n",
    "    data = pd.concat([train_copy, test_copy]).reset_index(drop=True)\n",
    "    data = data[train_copy.columns]\n",
    "    \n",
    "    # fix skew feature\n",
    "    skew_columns = ['price']\n",
    "\n",
    "    for c in skew_columns:\n",
    "        data[c] = np.log1p(data[c])\n",
    "    \n",
    "    if original:\n",
    "        # feature engineering\n",
    "        data['date'] = pd.to_datetime(data['date'])\n",
    "        data['yr_mo_sold'] = data['date'].dt.strftime('%Y-%m')\n",
    "        data['yr_sold'] = data['date'].dt.year\n",
    "        data['qt_sold'] = data['date'].dt.quarter\n",
    "        data['week_sold'] = data['date'].dt.week\n",
    "        data['dow_sold'] = data['date'].dt.dayofweek\n",
    "        data['yr_sold - yr_built'] = data['yr_sold'] - data['yr_built']\n",
    "        data['yr_sold - yr_renovated'] = data['yr_sold'] - data['yr_renovated']\n",
    "        data['yr_renovated - yr_built'] = data['yr_renovated'] - data['yr_built']\n",
    "        \n",
    "        data['yr_sold'] = data['yr_sold'].astype(str)\n",
    "        data['qt_sold'] = data['qt_sold'].astype(str)\n",
    "        data['week_sold'] = data['week_sold'].astype(str)\n",
    "        data['dow_sold'] = data['dow_sold'].astype(str)\n",
    "        data.drop(['date'], axis=1, inplace=True)\n",
    "\n",
    "        data['bedrooms + bathrooms'] = data['bedrooms'] + data['bathrooms']\n",
    "        data['bathrooms / bedrooms'] = data['bathrooms'] / data['bedrooms']\n",
    "        data.loc[np.isinf(data['bathrooms / bedrooms']), 'bathrooms / bedrooms'] = 0\n",
    "        data['bathrooms / bedrooms'].fillna(0, inplace=True)\n",
    "        data['sqft_living / bedrooms'] = data['sqft_living'] / data['bedrooms']\n",
    "        data.loc[np.isinf(data['sqft_living / bedrooms']), 'sqft_living / bedrooms'] = 0\n",
    "        data['sqft_living / bathrooms'] = data['sqft_living'] / data['bathrooms']\n",
    "        data.loc[np.isinf(data['sqft_living / bathrooms']), 'sqft_living / bathrooms'] = 0\n",
    "        data['sqft_living / floors'] = data['sqft_living'] / data['floors']\n",
    "        data.loc[np.isinf(data['sqft_living / floors']), 'sqft_living / floors'] = 0\n",
    "        data['sqft_lot / sqft_living'] = data['sqft_lot'] / data['sqft_living']\n",
    "        data['sqft_basement / sqft_above'] = data['sqft_basement'] / data['sqft_above']\n",
    "        data['sqft_lot15 / sqft_living15'] = data['sqft_lot15'] / data['sqft_living15']\n",
    "        data['has_basement'] = data['sqft_basement'] > 0\n",
    "        data['is_renovated'] = data['yr_renovated'] > 0\n",
    "        data['sqft_living_changed'] = data['sqft_living'] != data['sqft_living15']\n",
    "        data['sqft_lot_changed'] = data['sqft_lot'] != data['sqft_lot15']\n",
    "        data['sqft_living * grade'] = data['sqft_living'] * data['grade']\n",
    "        data['overall'] = data['grade'] + data['view'] + data['condition'] + data['waterfront'] + data['has_basement'] + data['is_renovated']\n",
    "        data['sqft_living * overall'] = data['sqft_living'] * data['overall']\n",
    "\n",
    "        data['zipcode'] = data['zipcode'].astype(str)\n",
    "        data['zipcode-3'] = data['zipcode'].str[2:3]\n",
    "        data['zipcode-4'] = data['zipcode'].str[3:4]\n",
    "        data['zipcode-5'] = data['zipcode'].str[4:5]\n",
    "        data['zipcode-34'] = data['zipcode'].str[2:4]\n",
    "        data['zipcode-45'] = data['zipcode'].str[3:5]\n",
    "        data['zipcode-35'] = data['zipcode-3'] + data['zipcode-5']\n",
    "\n",
    "        # pca for lat, long\n",
    "        coord = data[['lat','long']]\n",
    "        pca = PCA(n_components=2)\n",
    "        pca.fit(coord)\n",
    "\n",
    "        coord_pca = pca.transform(coord)\n",
    "\n",
    "        data['coord_pca1'] = coord_pca[:, 0]\n",
    "        data['coord_pca2'] = coord_pca[:, 1]\n",
    "\n",
    "        # kmeans for lat, long\n",
    "        kmeans = KMeans(n_clusters=72, random_state=RANDOM_SEED).fit(coord)\n",
    "        coord_cluster = kmeans.predict(coord)\n",
    "        data['coord_cluster'] = coord_cluster\n",
    "        data['coord_cluster'] = data['coord_cluster'].map(lambda x: 'cluster_' + str(x).rjust(2, '0'))\n",
    "\n",
    "        lat_med = data['lat'].median()\n",
    "        long_med = data['long'].median()\n",
    "\n",
    "        lat2 = data['lat'].values\n",
    "        long2 = data['long'].values\n",
    "\n",
    "        bearing_arr = bearing_array(lat_med, long_med, lat2, long2)\n",
    "\n",
    "        data['bearing_from_center'] = bearing_arr\n",
    "\n",
    "        qcut_count = 10\n",
    "        data['qcut_bearing'] = pd.qcut(data['bearing_from_center'], qcut_count, labels=range(qcut_count))\n",
    "        data['qcut_bearing'] = data['qcut_bearing'].astype(str)\n",
    "\n",
    "        # calculate grouped price\n",
    "        group_cols = ['grade','bedrooms','bathrooms','view','condition','waterfront']\n",
    "\n",
    "        for col in group_cols:\n",
    "            group_df = groupby_helper(data[data['data'] == 'train'], col, 'price', ['mean']).fillna(0)\n",
    "            data = data.merge(group_df, on=col, how='left').fillna(0)\n",
    "            \n",
    "        cat_cols = [\n",
    "            'yr_mo_sold','yr_sold','qt_sold','week_sold','dow_sold','coord_cluster',\n",
    "            'zipcode','zipcode-3','zipcode-4','zipcode-5','zipcode-34','zipcode-45','zipcode-35',\n",
    "            'qcut_bearing'\n",
    "        ]\n",
    "            \n",
    "        if do_ohe:\n",
    "            for col in cat_cols:\n",
    "                ohe_df = pd.get_dummies(data[[col]], prefix='ohe_'+col)\n",
    "                data.drop(col, axis=1, inplace=True)\n",
    "                data = pd.concat([data, ohe_df], axis=1)\n",
    "        else:\n",
    "            for col in cat_cols:\n",
    "                le = LabelEncoder()\n",
    "                data[col] = le.fit_transform(data[col])\n",
    "    else:\n",
    "        data = data[['id','price','data']]\n",
    "        \n",
    "    if nb_1km:\n",
    "        neighbor_1km_stat = pd.read_csv('aiffel/kaggle_kakr_housing/data/neighbor_1km_stat.csv')\n",
    "        data = data.merge(neighbor_1km_stat, on='id', how='left').fillna(0)\n",
    "        \n",
    "        if original:\n",
    "            data['sqft_living - nb_1km_sqft_living_mean'] = data['sqft_living'] - data['nb_1km_sqft_living_mean']\n",
    "            data['sqft_lot - nb_1km_sqft_lot_mean'] = data['sqft_lot'] - data['nb_1km_sqft_lot_mean']\n",
    "            data['bedrooms - nb_1km_bedrooms_mean'] = data['bedrooms'] - data['nb_1km_bedrooms_mean']\n",
    "            data['bathrooms - nb_1km_bathrooms_mean'] = data['bathrooms'] - data['nb_1km_bathrooms_mean']\n",
    "            data['grade - nb_1km_grade_mean'] = data['grade'] - data['nb_1km_grade_mean']\n",
    "            data['view - nb_1km_view_mean'] = data['view'] - data['nb_1km_view_mean']\n",
    "            data['condition - nb_1km_condition_mean'] = data['condition'] - data['nb_1km_condition_mean']\n",
    "    if nb_3km:\n",
    "        neighbor_3km_stat = pd.read_csv('aiffel/kaggle_kakr_housing/data/neighbor_3km_stat.csv')\n",
    "        data = data.merge(neighbor_3km_stat, on='id', how='left').fillna(0)\n",
    "        \n",
    "        if original:\n",
    "            data['sqft_living - nb_3km_sqft_living_mean'] = data['sqft_living'] - data['nb_3km_sqft_living_mean']\n",
    "            data['sqft_lot - nb_3km_sqft_lot_mean'] = data['sqft_lot'] - data['nb_3km_sqft_lot_mean']\n",
    "            data['bedrooms - nb_3km_bedrooms_mean'] = data['bedrooms'] - data['nb_3km_bedrooms_mean']\n",
    "            data['bathrooms - nb_3km_bathrooms_mean'] = data['bathrooms'] - data['nb_3km_bathrooms_mean']\n",
    "            data['grade - nb_3km_grade_mean'] = data['grade'] - data['nb_3km_grade_mean']\n",
    "            data['view - nb_3km_view_mean'] = data['view'] - data['nb_3km_view_mean']\n",
    "            data['condition - nb_3km_condition_mean'] = data['condition'] - data['nb_3km_condition_mean']\n",
    "    if nb_5km:\n",
    "        neighbor_5km_stat = pd.read_csv('aiffel/kaggle_kakr_housing/data/neighbor_5km_stat.csv')\n",
    "        data = data.merge(neighbor_5km_stat, on='id', how='left').fillna(0)\n",
    "        \n",
    "        if original:\n",
    "            data['sqft_living - nb_5km_sqft_living_mean'] = data['sqft_living'] - data['nb_5km_sqft_living_mean']\n",
    "            data['sqft_lot - nb_5km_sqft_lot_mean'] = data['sqft_lot'] - data['nb_5km_sqft_lot_mean']\n",
    "            data['bedrooms - nb_5km_bedrooms_mean'] = data['bedrooms'] - data['nb_5km_bedrooms_mean']\n",
    "            data['bathrooms - nb_5km_bathrooms_mean'] = data['bathrooms'] - data['nb_5km_bathrooms_mean']\n",
    "            data['grade - nb_5km_grade_mean'] = data['grade'] - data['nb_5km_grade_mean']\n",
    "            data['view - nb_5km_view_mean'] = data['view'] - data['nb_5km_view_mean']\n",
    "            data['condition - nb_5km_condition_mean'] = data['condition'] - data['nb_5km_condition_mean']\n",
    "    if n_5_nb:\n",
    "        nearest_5_neighbor_stat = pd.read_csv('aiffel/kaggle_kakr_housing/data/nearest_5_neighbor_stat.csv')\n",
    "        data = data.merge(nearest_5_neighbor_stat, on='id', how='left').fillna(0)\n",
    "        \n",
    "        if original:\n",
    "            data['sqft_living - n_5_nb_sqft_living_mean'] = data['sqft_living'] - data['n_5_nb_sqft_living_mean']\n",
    "            data['sqft_lot - n_5_nb_sqft_lot_mean'] = data['sqft_lot'] - data['n_5_nb_sqft_lot_mean']\n",
    "            data['bedrooms - n_5_nb_bedrooms_mean'] = data['bedrooms'] - data['n_5_nb_bedrooms_mean']\n",
    "            data['bathrooms - n_5_nb_bathrooms_mean'] = data['bathrooms'] - data['n_5_nb_bathrooms_mean']\n",
    "            data['grade - n_5_nb_grade_mean'] = data['grade'] - data['n_5_nb_grade_mean']\n",
    "            data['view - n_5_nb_view_mean'] = data['view'] - data['n_5_nb_view_mean']\n",
    "            data['condition - n_5_nb_condition_mean'] = data['condition'] - data['n_5_nb_condition_mean']\n",
    "    if n_10_nb:\n",
    "        nearest_10_neighbor_stat = pd.read_csv('aiffel/kaggle_kakr_housing/data/nearest_10_neighbor_stat.csv')\n",
    "        data = data.merge(nearest_10_neighbor_stat, on='id', how='left').fillna(0)\n",
    "        \n",
    "        if original:\n",
    "            data['sqft_living - n_10_nb_sqft_living_mean'] = data['sqft_living'] - data['n_10_nb_sqft_living_mean']\n",
    "            data['sqft_lot - n_10_nb_sqft_lot_mean'] = data['sqft_lot'] - data['n_10_nb_sqft_lot_mean']\n",
    "            data['bedrooms - n_10_nb_bedrooms_mean'] = data['bedrooms'] - data['n_10_nb_bedrooms_mean']\n",
    "            data['bathrooms - n_10_nb_bathrooms_mean'] = data['bathrooms'] - data['n_10_nb_bathrooms_mean']\n",
    "            data['grade - n_10_nb_grade_mean'] = data['grade'] - data['n_10_nb_grade_mean']\n",
    "            data['view - n_10_nb_view_mean'] = data['view'] - data['n_10_nb_view_mean']\n",
    "            data['condition - n_10_nb_condition_mean'] = data['condition'] - data['n_10_nb_condition_mean']\n",
    "    if n_20_nb:\n",
    "        nearest_20_neighbor_stat = pd.read_csv('aiffel/kaggle_kakr_housing/data/nearest_20_neighbor_stat.csv')\n",
    "        data = data.merge(nearest_20_neighbor_stat, on='id', how='left').fillna(0)\n",
    "        \n",
    "        if original:\n",
    "            data['sqft_living - n_20_nb_sqft_living_mean'] = data['sqft_living'] - data['n_20_nb_sqft_living_mean']\n",
    "            data['sqft_lot - n_20_nb_sqft_lot_mean'] = data['sqft_lot'] - data['n_20_nb_sqft_lot_mean']\n",
    "            data['bedrooms - n_20_nb_bedrooms_mean'] = data['bedrooms'] - data['n_20_nb_bedrooms_mean']\n",
    "            data['bathrooms - n_20_nb_bathrooms_mean'] = data['bathrooms'] - data['n_20_nb_bathrooms_mean']\n",
    "            data['grade - n_20_nb_grade_mean'] = data['grade'] - data['n_20_nb_grade_mean']\n",
    "            data['view - n_20_nb_view_mean'] = data['view'] - data['n_20_nb_view_mean']\n",
    "            data['condition - n_20_nb_condition_mean'] = data['condition'] - data['n_20_nb_condition_mean']\n",
    "                \n",
    "    if fix_skew:\n",
    "        ordinal_cols = [\n",
    "            'id','price','data',\n",
    "            'grade','overall','view','condition','waterfront','is_renovated','has_basement',\n",
    "        ]\n",
    "        if do_ohe:\n",
    "            exclude_cols = [col for col in data.columns if 'ohe_' in col] + ordinal_cols\n",
    "        else:\n",
    "            exclude_cols = cat_cols + ordinal_cols\n",
    "        numeric_feats = [col for col in data.columns if col not in exclude_cols]\n",
    "        skewed_feats = data[numeric_feats].apply(lambda x : skew(x.dropna())).sort_values(ascending=False)\n",
    "        skewness = pd.DataFrame({'skew' :skewed_feats})\n",
    "        skewness = skewness[abs(skewness) > 0.75]\n",
    "        skewed_features = skewness.index\n",
    "        lam = 0.15\n",
    "        for feat in skewed_features:\n",
    "            data[feat] = boxcox1p(data[feat], lam)\n",
    "            data[feat] = data[feat].fillna(0)\n",
    "    \n",
    "    df = data.drop(['id','price','data'], axis=1).copy()\n",
    "\n",
    "    train_len = data[data['data'] == 'train'].shape[0]\n",
    "    X_train = df.iloc[:train_len]\n",
    "    X_test = df.iloc[train_len:]\n",
    "    y_train = data[data['data'] == 'train']['price']\n",
    "    \n",
    "    if do_scale:\n",
    "        if do_ohe:\n",
    "            non_numeric_cols = [col for col in X_train.columns if 'ohe_' in col]\n",
    "            numeric_cols = [col for col in X_train.columns if 'ohe_' not in col]\n",
    "        else:\n",
    "            non_numeric_cols = cat_cols\n",
    "            numeric_cols = [col for col in X_train.columns if col not in cat_cols]\n",
    "        X_train_rb = X_train[numeric_cols].copy()\n",
    "        X_test_rb = X_test[numeric_cols].copy()\n",
    "\n",
    "        rb = RobustScaler()\n",
    "        X_train_rb = rb.fit_transform(X_train_rb)\n",
    "        X_test_rb = rb.transform(X_test_rb)\n",
    "\n",
    "        X_train_rb = pd.DataFrame(X_train_rb, index=X_train.index, columns=X_train[numeric_cols].columns)\n",
    "        X_test_rb = pd.DataFrame(X_test_rb, index=X_test.index, columns=X_test[numeric_cols].columns)\n",
    "        \n",
    "        X_train_rb = pd.concat([X_train[non_numeric_cols], X_train_rb], axis=1)\n",
    "        X_test_rb = pd.concat([X_test[non_numeric_cols], X_test_rb], axis=1)\n",
    "        \n",
    "        print(X_train_rb.shape, X_test_rb.shape, y_train.shape)\n",
    "        \n",
    "        return X_train_rb, X_test_rb, y_train\n",
    "    else :\n",
    "        print(X_train.shape, X_test.shape, y_train.shape)\n",
    "        return X_train, X_test, y_train\n",
    "\n",
    "def haversine_array(lat1, lng1, lat2, lng2): \n",
    "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2)) \n",
    "    AVG_EARTH_RADIUS = 6371 # in km \n",
    "    lat = lat2 - lat1 \n",
    "    lng = lng2 - lng1 \n",
    "    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2 \n",
    "    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d)) \n",
    "    return h\n",
    "\n",
    "def bearing_array(lat1, lng1, lat2, lng2): \n",
    "    AVG_EARTH_RADIUS = 6371 # in km \n",
    "    lng_delta_rad = np.radians(lng2 - lng1) \n",
    "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2)) \n",
    "    y = np.sin(lng_delta_rad) * np.cos(lat2) \n",
    "    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad) \n",
    "    return np.degrees(np.arctan2(y, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set a few plotting defaults\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "pd.options.display.max_rows = 10000\n",
    "pd.options.display.max_columns = 10000\n",
    "pd.options.display.max_colwidth = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "\n",
    "X_train_1km, X_test_1km, y_train = load_data(nb_1km=True, nb_3km=False, nb_5km=False,\n",
    "                                             n_5_nb=False, n_10_nb=False, n_20_nb=False,\n",
    "                                             original=False, do_scale=False, do_ohe=True)\n",
    "data_list.append((X_train_1km, X_test_1km))\n",
    "\n",
    "X_train_3km, X_test_3km, y_train = load_data(nb_1km=False, nb_3km=True, nb_5km=False,\n",
    "                                             n_5_nb=False, n_10_nb=False, n_20_nb=False,\n",
    "                                             original=False, do_scale=False, do_ohe=True)\n",
    "data_list.append((X_train_3km, X_test_3km))\n",
    "\n",
    "X_train_5km, X_test_5km, y_train = load_data(nb_1km=False, nb_3km=False, nb_5km=True,\n",
    "                                             n_5_nb=False, n_10_nb=False, n_20_nb=False,\n",
    "                                             original=False, do_scale=False, do_ohe=True)\n",
    "data_list.append((X_train_5km, X_test_5km))\n",
    "\n",
    "X_train_5_nn, X_test_5_nn, y_train = load_data(nb_1km=False, nb_3km=False, nb_5km=False,\n",
    "                                               n_5_nb=True, n_10_nb=False, n_20_nb=False,\n",
    "                                               original=False, do_scale=False, do_ohe=True)\n",
    "data_list.append((X_train_5_nn, X_test_5_nn))\n",
    "\n",
    "X_train_10_nn, X_test_10_nn, y_train = load_data(nb_1km=False, nb_3km=False, nb_5km=False,\n",
    "                                                 n_5_nb=False, n_10_nb=True, n_20_nb=False,\n",
    "                                                 original=False, do_scale=False, do_ohe=True)\n",
    "data_list.append((X_train_10_nn, X_test_10_nn))\n",
    "\n",
    "X_train_20_nn, X_test_20_nn, y_train = load_data(nb_1km=False, nb_3km=False, nb_5km=False,\n",
    "                                                 n_5_nb=False, n_10_nb=False, n_20_nb=True,\n",
    "                                                 original=False, do_scale=False, do_ohe=True)\n",
    "data_list.append((X_train_20_nn, X_test_20_nn))\n",
    "\n",
    "X_train_ori, X_test_ori, y_train = load_data(nb_1km=False, nb_3km=False, nb_5km=False,\n",
    "                                             n_5_nb=False, n_10_nb=False, n_20_nb=False,\n",
    "                                             original=True, do_scale=False, do_ohe=True)\n",
    "data_list.append((X_train_ori, X_test_ori))\n",
    "\n",
    "X_train_full, X_test_full, y_train = load_data(nb_1km=True, nb_3km=True, nb_5km=True,\n",
    "                                               n_5_nb=True, n_10_nb=True, n_20_nb=True,\n",
    "                                               original=True, do_scale=False, do_ohe=True)\n",
    "data_list.append((X_train_full, X_test_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_flag = False\n",
    "\n",
    "if run_flag:\n",
    "    model_list = []\n",
    "\n",
    "    lgb_param = {\n",
    "        'objective': 'regression',\n",
    "        'learning_rate': 0.01,\n",
    "        'max_depth': 20,\n",
    "        'num_leaves': 63,\n",
    "        'min_data_in_leaf': 30,\n",
    "        'bagging_fraction': 0.7,\n",
    "        'bagging_freq': 1,\n",
    "        'feature_fraction': 0.2,\n",
    "        'seed': RANDOM_SEED,\n",
    "        'metric': ['rmse'],\n",
    "    }\n",
    "    lgb_model = LgbmWrapper(params=lgb_param, num_rounds=100000, ealry_stopping=200, verbose_eval=False)\n",
    "    model_list.append(lgb_model)\n",
    "\n",
    "    lgb_param2 = {\n",
    "        'objective': 'regression',\n",
    "        'learning_rate': 0.01,\n",
    "        'max_depth': 10,\n",
    "        'num_leaves': 31,\n",
    "        'min_data_in_leaf': 30,\n",
    "        'bagging_fraction': 0.7,\n",
    "        'bagging_freq': 1,\n",
    "        'feature_fraction': 0.2,\n",
    "        'seed': RANDOM_SEED,\n",
    "        'metric': ['rmse'],\n",
    "    }\n",
    "    lgb_model2 = LgbmWrapper(params=lgb_param2, num_rounds=100000, ealry_stopping=200, verbose_eval=False)\n",
    "    model_list.append(lgb_model2)\n",
    "\n",
    "    lgb_param3 = {\n",
    "        'objective': 'regression',\n",
    "        'learning_rate': 0.01,\n",
    "        'max_depth': 3,\n",
    "        'num_leaves': 7,\n",
    "        'min_data_in_leaf': 30,\n",
    "        'bagging_fraction': 0.7,\n",
    "        'bagging_freq': 1,\n",
    "        'feature_fraction': 0.2,\n",
    "        'seed': RANDOM_SEED,\n",
    "        'metric': ['rmse'],\n",
    "    }\n",
    "    lgb_model3 = LgbmWrapper(params=lgb_param3, num_rounds=100000, ealry_stopping=200, verbose_eval=False)\n",
    "    model_list.append(lgb_model3)\n",
    "\n",
    "    xgb_param = {\n",
    "        'eval_metric': 'rmse',\n",
    "        'seed': RANDOM_SEED,\n",
    "        'eta': 0.01,\n",
    "        'max_depth': 20,\n",
    "        'subsample': 0.7,\n",
    "        'colsample_bytree': 0.5,\n",
    "        'silent': 1,\n",
    "    }\n",
    "    xgb_model = XgbWrapper(params=xgb_param, num_rounds=100000, ealry_stopping=200, verbose_eval=False)\n",
    "    model_list.append(xgb_model)\n",
    "\n",
    "    xgb_param2 = {\n",
    "        'eval_metric': 'rmse',\n",
    "        'seed': RANDOM_SEED,\n",
    "        'eta': 0.01,\n",
    "        'max_depth': 10,\n",
    "        'subsample': 0.7,\n",
    "        'colsample_bytree': 0.5,\n",
    "        'silent': 1,\n",
    "    }\n",
    "    xgb_model2 = XgbWrapper(params=xgb_param2, num_rounds=100000, ealry_stopping=200, verbose_eval=False)\n",
    "    model_list.append(xgb_model2)\n",
    "\n",
    "    xgb_param3 = {\n",
    "        'eval_metric': 'rmse',\n",
    "        'seed': RANDOM_SEED,\n",
    "        'eta': 0.01,\n",
    "        'max_depth': 3,\n",
    "        'subsample': 0.7,\n",
    "        'colsample_bytree': 0.5,\n",
    "        'silent': 1,\n",
    "    }\n",
    "    xgb_model3 = XgbWrapper(params=xgb_param3, num_rounds=100000, ealry_stopping=200, verbose_eval=False)\n",
    "    model_list.append(xgb_model3)\n",
    "\n",
    "    rf_param = {\n",
    "        'n_estimators': 1000,\n",
    "        'max_depth': 20,\n",
    "        'max_features': 0.6,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': RANDOM_SEED\n",
    "    }\n",
    "    rf_model = SklearnWrapper(RandomForestRegressor, params=rf_param)\n",
    "    model_list.append(rf_model)\n",
    "\n",
    "    rf_param2 = {\n",
    "        'n_estimators': 1000,\n",
    "        'max_depth': 3,\n",
    "        'max_features': 0.6,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': RANDOM_SEED\n",
    "    }\n",
    "    rf_model2 = SklearnWrapper(RandomForestRegressor, params=rf_param2)\n",
    "    model_list.append(rf_model2)\n",
    "\n",
    "    ridge_param = {'alpha': 1e-3, 'normalize': True, 'max_iter': 1e7, 'random_state': RANDOM_SEED}\n",
    "    ridge_model = SklearnWrapper(Ridge, params=ridge_param)\n",
    "    model_list.append(ridge_model)\n",
    "\n",
    "    X_train_single, X_test_single, cv_score_single = stacking(data_list, y_train, model_list, eval_func=rmse_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_single = pd.read_csv('aiffel/kaggle_kakr_housing/data/x_train_single.csv')\n",
    "X_test_single = pd.read_csv('aiffel/kaggle_kakr_housing/data/x_test_single.csv')\n",
    "cv_score_single_df = pd.read_csv('aiffel/kaggle_kakr_housing/data/cv_score_single.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_score_single_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_score_single_df.set_index('name').plot.bar(figsize=(16,8))\n",
    "plt.ylabel('Single Model CV Score');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_single.shape, X_test_single.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stage1 = pd.concat([\n",
    "    X_train_single,\n",
    "    X_train_full[['nb_1km_price_mean','nb_3km_price_mean','nb_5km_price_mean',\n",
    "                  'n_5_nb_price_mean','n_10_nb_price_mean','n_20_nb_price_mean']]\n",
    "], axis=1)\n",
    "\n",
    "X_test_stage1 = pd.concat([\n",
    "    X_test_single,\n",
    "    X_test_full[['nb_1km_price_mean','nb_3km_price_mean','nb_5km_price_mean',\n",
    "                 'n_5_nb_price_mean','n_10_nb_price_mean','n_20_nb_price_mean']].reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "X_train_stage1.shape, X_test_stage1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='selu', input_dim=input_dim,\n",
    "                    kernel_initializer=initializers.he_normal(seed=RANDOM_SEED),\n",
    "                    bias_initializer=initializers.Constant(0.01)))\n",
    "    model.add(Dense(32, activation='selu', \n",
    "                    kernel_initializer=initializers.he_normal(seed=RANDOM_SEED),\n",
    "                    bias_initializer=initializers.Constant(0.01)))\n",
    "    model.add(Dense(16, activation='selu',\n",
    "                    kernel_initializer=initializers.he_normal(seed=RANDOM_SEED),\n",
    "                    bias_initializer=initializers.Constant(0.01)))\n",
    "    model.add(Dense(8, activation='selu',\n",
    "                    kernel_initializer=initializers.he_normal(seed=RANDOM_SEED),\n",
    "                    bias_initializer=initializers.Constant(0.01)))\n",
    "    model.add(Dense(1,\n",
    "                    kernel_initializer=initializers.he_normal(seed=RANDOM_SEED),\n",
    "                    bias_initializer=initializers.Constant(0.01)))\n",
    "\n",
    "\n",
    "    optimizer = optimizers.RMSprop(lr=0.001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mse'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "patient = 200\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=patient, mode='min', verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=patient/2, min_lr=0.00001, verbose=1, mode='min')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_flag:\n",
    "    model_list_second = []\n",
    "\n",
    "    keras_model = KerasWrapper(create_model, epochs=100000, batch_size=512, callbacks=callbacks, use_avg_oof=True)\n",
    "    model_list_second.append(keras_model)\n",
    "\n",
    "    et_param = {\n",
    "        'n_estimators': 1000,\n",
    "        'max_depth': 20,\n",
    "        'max_features': 0.6,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': RANDOM_SEED\n",
    "    }\n",
    "    et_model = SklearnWrapper(ExtraTreesRegressor, params=et_param)\n",
    "    model_list_second.append(et_model)\n",
    "\n",
    "    et_param2 = {\n",
    "        'n_estimators': 1000,\n",
    "        'max_depth': 3,\n",
    "        'max_features': 0.6,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': RANDOM_SEED\n",
    "    }\n",
    "    et_model2 = SklearnWrapper(ExtraTreesRegressor, params=et_param2)\n",
    "    model_list_second.append(et_model2)\n",
    "\n",
    "    lgb_param = {\n",
    "        'objective': 'regression',\n",
    "        'learning_rate': 0.01,\n",
    "        'max_depth': 20,\n",
    "        'num_leaves': 63,\n",
    "        'min_data_in_leaf': 30,\n",
    "        'bagging_fraction': 0.7,\n",
    "        'bagging_freq': 1,\n",
    "        'feature_fraction': 0.2,\n",
    "        'seed': RANDOM_SEED,\n",
    "        'metric': ['rmse'],\n",
    "    }\n",
    "    lgb_model = LgbmWrapper(params=lgb_param, num_rounds=100000, ealry_stopping=200, verbose_eval=False)\n",
    "    model_list_second.append(lgb_model)\n",
    "\n",
    "    lgb_param2 = {\n",
    "        'objective': 'regression',\n",
    "        'learning_rate': 0.01,\n",
    "        'max_depth': 10,\n",
    "        'num_leaves': 31,\n",
    "        'min_data_in_leaf': 30,\n",
    "        'bagging_fraction': 0.7,\n",
    "        'bagging_freq': 1,\n",
    "        'feature_fraction': 0.2,\n",
    "        'seed': RANDOM_SEED,\n",
    "        'metric': ['rmse'],\n",
    "    }\n",
    "    lgb_model2 = LgbmWrapper(params=lgb_param2, num_rounds=100000, ealry_stopping=200, verbose_eval=False)\n",
    "    model_list_second.append(lgb_model2)\n",
    "\n",
    "    lgb_param3 = {\n",
    "        'objective': 'regression',\n",
    "        'learning_rate': 0.01,\n",
    "        'max_depth': 3,\n",
    "        'num_leaves': 7,\n",
    "        'min_data_in_leaf': 30,\n",
    "        'bagging_fraction': 0.7,\n",
    "        'bagging_freq': 1,\n",
    "        'feature_fraction': 0.2,\n",
    "        'seed': RANDOM_SEED,\n",
    "        'metric': ['rmse'],\n",
    "    }\n",
    "    lgb_model3 = LgbmWrapper(params=lgb_param3, num_rounds=100000, ealry_stopping=200, verbose_eval=False)\n",
    "    model_list_second.append(lgb_model3)\n",
    "\n",
    "    xgb_param = {\n",
    "        'eval_metric': 'rmse',\n",
    "        'seed': RANDOM_SEED,\n",
    "        'eta': 0.01,\n",
    "        'max_depth': 20,\n",
    "        'subsample': 0.7,\n",
    "        'colsample_bytree': 0.5,\n",
    "        'silent': 1,\n",
    "    }\n",
    "    xgb_model = XgbWrapper(params=xgb_param, num_rounds=100000, ealry_stopping=200, verbose_eval=False)\n",
    "    model_list_second.append(xgb_model)\n",
    "\n",
    "    xgb_param2 = {\n",
    "        'eval_metric': 'rmse',\n",
    "        'seed': RANDOM_SEED,\n",
    "        'eta': 0.01,\n",
    "        'max_depth': 10,\n",
    "        'subsample': 0.7,\n",
    "        'colsample_bytree': 0.5,\n",
    "        'silent': 1,\n",
    "    }\n",
    "    xgb_model2 = XgbWrapper(params=xgb_param2, num_rounds=100000, ealry_stopping=200, verbose_eval=False)\n",
    "    model_list_second.append(xgb_model2)\n",
    "\n",
    "    xgb_param3 = {\n",
    "        'eval_metric': 'rmse',\n",
    "        'seed': RANDOM_SEED,\n",
    "        'eta': 0.01,\n",
    "        'max_depth': 3,\n",
    "        'subsample': 0.7,\n",
    "        'colsample_bytree': 0.5,\n",
    "        'silent': 1,\n",
    "    }\n",
    "    xgb_model3 = XgbWrapper(params=xgb_param3, num_rounds=100000, ealry_stopping=200, verbose_eval=False)\n",
    "    model_list_second.append(xgb_model3)\n",
    "\n",
    "    rf_param = {\n",
    "        'n_estimators': 1000,\n",
    "        'max_depth': 20,\n",
    "        'max_features': 0.6,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': RANDOM_SEED\n",
    "    }\n",
    "    rf_model = SklearnWrapper(RandomForestRegressor, params=rf_param)\n",
    "    model_list_second.append(rf_model)\n",
    "\n",
    "    rf_param2 = {\n",
    "        'n_estimators': 1000,\n",
    "        'max_depth': 3,\n",
    "        'max_features': 0.6,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': RANDOM_SEED\n",
    "    }\n",
    "    rf_model2 = SklearnWrapper(RandomForestRegressor, params=rf_param2)\n",
    "    model_list_second.append(rf_model2)\n",
    "\n",
    "    ridge_param = {'alpha': 1e-10, 'normalize': True, 'max_iter': 1e7, 'random_state': RANDOM_SEED}\n",
    "    ridge_model = SklearnWrapper(Ridge, params=ridge_param)\n",
    "    model_list_second.append(ridge_model)\n",
    "\n",
    "    gbr_param = {\n",
    "        'n_estimators': 1000,\n",
    "        'learning_rate':0.1,\n",
    "        'max_depth': 20,\n",
    "        'subsample': 0.7,\n",
    "        'max_features': 0.6,\n",
    "        'random_state': RANDOM_SEED\n",
    "    }\n",
    "    gbr_model = SklearnWrapper(GradientBoostingRegressor, params=gbr_param)\n",
    "    model_list_second.append(gbr_model)\n",
    "\n",
    "    gbr_param2 = {\n",
    "        'n_estimators': 1000,\n",
    "        'learning_rate':0.1,\n",
    "        'max_depth': 10,\n",
    "        'subsample': 0.7,\n",
    "        'max_features': 0.6,\n",
    "        'random_state': RANDOM_SEED\n",
    "    }\n",
    "    gbr_model2 = SklearnWrapper(GradientBoostingRegressor, params=gbr_param2)\n",
    "    model_list_second.append(gbr_model2)\n",
    "\n",
    "    gbr_param3 = {\n",
    "        'n_estimators': 1000,\n",
    "        'learning_rate':0.1,\n",
    "        'max_depth': 2,\n",
    "        'subsample': 0.7,\n",
    "        'max_features': 0.6,\n",
    "        'random_state': RANDOM_SEED\n",
    "    }\n",
    "    gbr_model3 = SklearnWrapper(GradientBoostingRegressor, params=gbr_param3)\n",
    "    model_list_second.append(gbr_model3)\n",
    "\n",
    "    lasso_param = {'alpha':1e-6, 'normalize':True, 'max_iter':1e7, 'random_state':RANDOM_SEED}\n",
    "    lasso_model = SklearnWrapper(Lasso, params=lasso_param)\n",
    "    model_list_second.append(lasso_model)\n",
    "\n",
    "    elastic_param = {'alpha':1e-6, 'normalize':True, 'max_iter':1e5, 'random_state':RANDOM_SEED, 'l1_ratio':0.8}\n",
    "    elastic_model = SklearnWrapper(ElasticNet, params=elastic_param)\n",
    "    model_list_second.append(elastic_model)\n",
    "\n",
    "    svr_param = {'C':1e3, 'epsilon':0.001, 'gamma':1e-4}\n",
    "    svr_model = SklearnWrapper(SVR, params=svr_param)\n",
    "    model_list_second.append(svr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_flag:\n",
    "    data_list_stage1 = [(X_train_stage1, X_test_stage1)]\n",
    "    X_train_stage2, X_test_stage2, cv_score_stage1 = stacking(data_list_stage1, y_train, model_list_second,\n",
    "                                                              eval_func=rmse_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stage2 = pd.read_csv('aiffel/kaggle_kakr_housing/data/x_train_stage2.csv')\n",
    "X_test_stage2 = pd.read_csv('aiffel/kaggle_kakr_housing/data/x_test_stage2.csv')\n",
    "cv_score_stage1_df = pd.read_csv('aiffel/kaggle_kakr_housing/data6/cv_score_stage1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_score_stage1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_score_stage1_df.set_index('name').plot.bar(figsize=(16,8))\n",
    "plt.ylabel('Stage 1 CV Score');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_l3_param = {\n",
    "    'alpha': 1e-10,\n",
    "    'normalize': True,\n",
    "    'max_iter': 1e7,\n",
    "    'random_state': RANDOM_SEED\n",
    "}\n",
    "\n",
    "ridge_l3_model = SklearnWrapper(Ridge, params=ridge_l3_param, use_avg_oof=True)\n",
    "\n",
    "ridge_l3_train, ridge_l3_test, ridge_l3_cv_score = get_oof(ridge_l3_model, X_train_stage2, y_train, X_test_stage2,\n",
    "                                                           rmse_exp, NFOLDS=5, kfold_random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_l3_param = {'alpha':1e-7, 'normalize':True, 'max_iter':1e7, 'random_state':RANDOM_SEED}\n",
    "\n",
    "lasso_l3_model = SklearnWrapper(Lasso, params=lasso_l3_param, use_avg_oof=True)\n",
    "lasso_l3_train, lasso_l3_test, lasso_l3_cv_score = get_oof(lasso_l3_model, X_train_stage2, y_train, X_test_stage2,\n",
    "                                                           rmse_exp, NFOLDS=5, kfold_random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_l3_param = {'alpha':1e-8, 'normalize':True, 'max_iter':1e6, 'random_state':RANDOM_SEED, 'l1_ratio':0.8}\n",
    "\n",
    "elastic_l3_model = SklearnWrapper(ElasticNet, params=elastic_l3_param, use_avg_oof=True)\n",
    "elastic_l3_train, elastic_l3_test, elastic_l3_cv_score = get_oof(elastic_l3_model, X_train_stage2, y_train, X_test_stage2,\n",
    "                                                                 rmse_exp, NFOLDS=5, kfold_random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_l3_param = {'alpha':1e-8, 'normalize':True, 'max_iter':1e6, 'random_state':RANDOM_SEED, 'l1_ratio':0.8}\n",
    "\n",
    "elastic_l3_model = SklearnWrapper(ElasticNet, params=elastic_l3_param, use_avg_oof=True)\n",
    "elastic_l3_train, elastic_l3_test, elastic_l3_cv_score = get_oof(elastic_l3_model, X_train_stage2, y_train, X_test_stage2,\n",
    "                                                                 rmse_exp, NFOLDS=5, kfold_random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_l3_param = {'C':1e3, 'epsilon':0.001, 'gamma':1e-4}\n",
    "\n",
    "svr_l3_model = SklearnWrapper(SVR, params=svr_l3_param, use_avg_oof=True)\n",
    "svr_l3_train, svr_l3_test, svr_l3_cv_score = get_oof(svr_l3_model, X_train_stage2, y_train, X_test_stage2,\n",
    "                                                     rmse_exp, NFOLDS=5, kfold_random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_score_stage2_df = pd.read_csv('aiffel/kaggle_kakr_housing/data/cv_score_stage2.csv')\n",
    "cv_score_stage2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_score_stage2_df.set_index('name').plot.bar(figsize=(16,8))\n",
    "plt.ylabel('Stage 2 CV Score');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# public score: 98316.65734, private score: 99336.33652\n",
    "\n",
    "avg_pred = elastic_l3_test\n",
    "\n",
    "test = pd.read_csv('aiffel/kaggle_kakr_housing/data/test.csv')\n",
    "\n",
    "output = f'stacking_{datetime.now().strftime(\"%Y%m%d%H%M%S\")}.csv'\n",
    "print(output)\n",
    "\n",
    "submission = pd.DataFrame({'id': test['id'], 'price': np.expm1(avg_pred.ravel())})\n",
    "submission.to_csv(output, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
